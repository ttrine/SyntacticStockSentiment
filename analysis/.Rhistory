onlySentiment$tweetBody[onlySentiment$numWords==1][11:20]
onlySentiment$tweetBody[onlySentiment$numWords==2][11:20]
onlySentiment$tweetBody[onlySentiment$numWords==2][1:10]
onlySentiment[onlySentiment$numWords==2][1:10]
onlySentiment[onlySentiment$numWords==2,1:10]
onlySentiment[1:10,onlySentiment$numWords==2]
onlySentiment$tweetBody[onlySentiment$numWords==2][11:20]
onlySentiment[onlySentiment$numWords==2][1:10]
onlySentiment[,onlySentiment$numWords==2][1:10]
onlySentiment[onlySentiment$numWords==2,][1:10]
onlySentiment[onlySentiment$numWords==2]
onlySentiment[onlySentiment$numWords==2,]
onlySentiment[onlySentiment$numWords==2,][1:10,]
plot(table(onlySentiment$numWords[onlySentiment$sentiment=="bullish"]))
plot(table(onlySentiment$numWords[onlySentiment$sentiment=="bearish"]))
plot(table(onlySentiment$numWords[onlySentiment$sentiment=="bullish"]))
plot(table(onlySentiment$numWords[onlySentiment$sentiment=="bearish"]))
plot(table(onlySentiment$numWords[onlySentiment$sentiment=="bullish"]))
plot(table(onlySentiment$numWords[onlySentiment$sentiment=="bearish"]))
plot(table(onlySentiment$numWords[onlySentiment$sentiment=="bullish"]))
plot(table(onlySentiment$numWords[onlySentiment$sentiment=="bearish"]))
plot(table(onlySentiment$numWords[onlySentiment$sentiment=="bullish"]))
onlySentiment[onlySentiment$numWords==2,][1:10]
onlySentiment[onlySentiment$numWords==2,][1:10,1]
onlySentiment[onlySentiment$numWords==2,][1:10,]
onlySentiment[onlySentiment$numWords==2,][1:10,tweetBody]
onlySentiment[onlySentiment$numWords==2,][11:30,]
tweetData <- read.csv("~/Documents/stockMartket/analysis/data/tweetData.csv", stringsAsFactors=FALSE)
View(tweetData)
tweetData$numWords <- sapply(strsplit(tweetData$tweetBody," +"),length) # count words
onlySentiment <- tweetData[tweetData$sentiment!='',]                    # isolate labeled tweets
plot(table(tweetData$numWords))                                         # these two plots
plot(table(onlySentiment$numWords))                                     # look the same
tweetData[tweetData$numWords==1]
tweetData[tweetData$numWords==1,]
tweetData[,tweetData$numWords==1]
tweetData$tweetBody[tweetData$numWords==1]
tweetData$tweetBody[tweetData$numWords==0]
which(tweetData$numWords==1)
tweetData <- read.csv("~/Documents/stockMartket/analysis/data/tweetData.csv", stringsAsFactors=FALSE)
tweetData$numWords <- sapply(strsplit(tweetData$tweetBody," +"),length) # count words
onlySentiment <- tweetData[tweetData$sentiment!='',]                    # isolate labeled tweets
plot(table(tweetData$numWords))                                         # these two plots
plot(table(onlySentiment$numWords))                                     # look the same
length(tweetData$tweetBodies[tweetData$numWords==2])
tweetData$tweetBodies[tweetData$numWords==2]
tweetData$numWords <- sapply(strsplit(tweetData$tweetBody," +"),length) # count words
onlySentiment <- tweetData[tweetData$sentiment!='',]                    # isolate labeled tweets
tweetData$tweetBodies[tweetData$numWords==2]
View(tweetData)
tweetData$tweetBody[tweetData$numWords==2]
length(tweetData$tweetBody[tweetData$numWords==2])
twoGrams<-tweetData$[tweetData$numWords==2,]
twoGrams<-tweetData[tweetData$numWords==2,]
sentimentWords <- strsplit(onlySentiment$tweetBody, " +")
sentimentWords[[1]]
s <- sentimentWords[[1]]
?strcontains
?containts
?contains
?substr
s[s!="LINKREPLACE"]
grepl(s,'CASHTAG')
grepl(s,'CASHTAG',fixed=TRUE)
?grepl
grepl('CASHTAG',s,fixed=TRUE)
s[s!="LINKREPLACE" & !grepl('CASHTAG',s,fixed=TRUE)]
remNeutral <- function(s) s[s!="LINKREPLACE" & !grepl('CASHTAG',s,fixed=TRUE)]
sapply(sentimentWords, remNeutral)
onlySentiment$filteredWords <- sapply(sentimentWords, remNeutral)
View(onlySentiment)
onlySentiment$filteredWords[1]
length(onlySentiment$filteredWords[1])
length(onlySentiment$filteredWords[56])
onlySentiment$filteredWords[56]
onlySentiment$filteredWords[56][[1]]
length(onlySentiment$filteredWords[56][[1]])
onlySentiment$filteredWordLens <- sapply(onlySentiment$filteredWords,length)
View(onlySentiment)
length(onlySentiment$filteredWordLens[onlySentiment$filteredWordLens==1])
length(unique(onlySentiment$filteredWordLens[onlySentiment$filteredWordLens==1]))
onlySentiment$filteredWordLens[onlySentiment$filteredWordLens==1][1:10]
onlySentiment$filteredWords[onlySentiment$filteredWordLens==1][1:10]
length(unique(onlySentiment$filteredWords[onlySentiment$filteredWordLens==1]))
length((onlySentiment$filteredWords[onlySentiment$filteredWordLens==1])
)
length(unique(tolower(onlySentiment$filteredWords[onlySentiment$filteredWordLens==1])))
length(unique(gsub('[[:punct:]]', '', tolower(onlySentiment$filteredWords[onlySentiment$filteredWordLens==1]))))
unique(gsub('[[:punct:]]', '', tolower(onlySentiment$filteredWords[onlySentiment$filteredWordLens==1])))
deez <- unique(gsub('[[:punct:]]', '', tolower(onlySentiment$filteredWords[onlySentiment$filteredWordLens==1])))
?unique
singles <- onlySentiment[onlySentiment$filteredWordLens==1]
singles <- onlySentiment[onlySentiment$filteredWordLens==1.]
singles <- onlySentiment[onlySentiment$filteredWordLens==1,]
singles$normalized <- gsub('[[:punct:]]', '', tolower(filteredWords[[1]]))
singles$normalized <- gsub('[[:punct:]]', '', tolower(singles$filteredWords[[1]]))
View(singles)
singles$normalized <- gsub('[[:punct:]]', '', tolower(singles$filteredWords))
View(singles)
table(singles$normalized)
plot(table(singles$normalized))
sort(table(singles$normalized))
plot(sort(table(singles$normalized)))
?sort
plot(sort(table(singles$normalized), decreasing=TRUE))
sort(table(singles$normalized), decreasing=TRUE)[1:200]
library(plyr)
singles$sentimInt <- ifelse(singles$sentiment=="bullish",1,-1)
View(singles)
ddply(singles,c(normalized),summarize,averageSentiment=sum(sentimInt)/length(sentimInt))
ddply(singles,c("normalized"),summarize,averageSentiment=sum(sentimInt)/length(sentimInt))
composite <- ddply(singles,c("normalized"),summarize,mean=sum(sentimInt)/length(sentimInt),stddev=sd(sentimInt))
View(composite)
composite <- ddply(singles,c("normalized"),summarize,frequency=length(sentimInt),mean=sum(sentimInt)/length(sentimInt),stddev=sd(sentimInt))
View(composite)
composite <- composite[composite$normalized!=" "]
composite <- composite[composite$normalized!=" ",]
composite <- composite[composite$normalized!="",]
?order
composite[with(composite, order(-frequency)), ]
View(composite[with(composite, order(-frequency)), ])
composite <- composite[with(composite, order(-frequency)), ]
table(composite$mean)
tweetData <- read.csv("~/Documents/stockMartket/analysis/data/tweetData.csv", stringsAsFactors=FALSE)
singles$sentiment <- ifelse(singles$sentiment=="bullish",               # sentiment to int
ifelse(singles$sentiment=="bearish",-1),NULL)
singles$sentiment <- ifelse(singles$sentiment=="bullish",               # sentiment to int
ifelse(singles$sentiment=="bearish",-1,NULL))
tweetData$sentiment <- ifelse(tweetData$sentiment=="bullish",               # sentiment to int
ifelse(tweetData$sentiment=="bearish",-1,NULL))
View(tweetData)
tweetData$sentiment <- ifelse(tweetData$sentiment=="bullish",1,         # sentiment to int
ifelse(tweetData$sentiment=="bearish",-1,NULL))
View(tweetData)
tweetData$sentiment
tweetData$sentiment == "bullish"
ifelse(tweetData$sentiment == "bullish",1,NULL)
ifelse(tweetData$sentiment == "bullish",1,0)
ifelse(tweetData$sentiment == "bullish",1,-Inf)
ifelse(tweetData$sentiment == "bullish",1,ifelse(tweetData$sentiment=="bearish",-1,Inf))
tweetData$sentiment <- ifelse(tweetData$sentiment == "bullish",1,
ifelse(tweetData$sentiment=="bearish",-1,Inf))
View(tweetData)
tweetData$numWords <- sapply(strsplit(tweetData$tweetBody," +"),length) # count words
onlySentiment <- tweetData[tweetData$sentiment!=Inf,]                    # isolate labeled tweets
plot(table(tweetData$numWords))                                         # these two plots
plot(table(onlySentiment$numWords))                                     # look the same
remNeutral <- function(s) s[s!="LINKREPLACE" & !grepl('CASHTAG',s,fixed=TRUE)]
onlySentiment$filteredWords <- sapply(sentimentWords, remNeutral)
singles <- onlySentiment[onlySentiment$filteredWordLens==1,]
singles$normalized <- gsub('[[:punct:]]', '', tolower(singles$filteredWords))
composite <- ddply(singles,c("normalized"),summarize, # aggregate by normalized wordcount
frequency=length(sentimInt)
mean=sum(sentimInt)/length(sentimInt),
stddev=sd(sentimInt)
)
composite <- composite[composite$normalized!="",] # ignore frequency of blanks
composite <- composite[with(composite, order(-frequency)), ] # sort descending
composite <- ddply(singles,c("normalized"),summarize, # aggregate by normalized wordcount
frequency=length(sentiment)
mean=sum(sentimInt)/length(sentiment),
stddev=sd(sentiment)
)
View(singles)
onlySentiment$filteredWords <- sapply(sentimentWords, remNeutral)
View(onlySentiment)
onlySentiment$filteredWordLens <- sapply(onlySentiment$filteredWords,length)
singles <- onlySentiment[onlySentiment$filteredWordLens==1,]
View(singles)
composite <- ddply(singles,c("normalized"),summarize, # aggregate by normalized wordcount
frequency=length(sentiment)
mean=sum(sentimInt)/length(sentiment),
stddev=sd(sentiment)
)
composite <- composite[composite$normalized!="",] # ignore frequency of blanks
composite <- composite[with(composite, order(-frequency)), ] # sort descending
singles$normalized <- gsub('[[:punct:]]', '', tolower(singles$filteredWords))
View(singles)
composite <- ddply(singles,c("normalized"),summarize, # aggregate by normalized wordcount
frequency=length(sentiment)
mean=sum(sentimInt)/length(sentiment),
stddev=sd(sentiment)
)
composite <- ddply(singles,c("normalized"),summarize, # aggregate by normalized wordcount
frequency=length(sentiment)
mean=sum(sentiment)/length(sentiment),
stddev=sd(sentiment)
)
composite <- ddply(singles,c("normalized"),summarize, # aggregate by normalized wordcount
frequency=length(sentiment)
mean=sum(sentiment)/length(sentiment),
stddev=sd(sentiment)
)
View(singles)
composite <- ddply(singles,c("normalized","sentiment"),summarize, # aggregate by normalized wordcount
frequency=length(sentiment)
mean=sum(sentiment)/length(sentiment),
stddev=sd(sentiment)
)
composite <- ddply(singles,c("normalized"),summarize, # aggregate by normalized wordcount
frequency=length(sentiment),
mean=sum(sentiment)/length(sentiment),
stddev=sd(sentiment)
)
View(composite)
composite <- composite[composite$normalized!="",] # ignore frequency of blanks
composite <- composite[with(composite, order(-frequency)), ] # sort descending
View(composite)
plot(composite$mean)
plot(composite$mean){1:200}
plot(composite$mean)[1:200]
plot(composite$mean[1:200])
plot(composite$sd[1:200])
plot(composite$stddev[1:200])
plot(composite$stddev)
plot(composite$stddev[1:250])
plot(composite$stddev[1:225])
plot(composite$stddev[1:220])
plot(composite$stddev[1:215])
plot(composite$stddev[1:210])
plot(composite$stddev[1:205])
plot(composite$stddev[1:200])
plot(composite$stddev[1:200],composite$frequency)
plot(composite$stddev[1:200],composite$frequency[1:200])
plot(composite$frequency[1:200],composite$stddev[1:200])
plot(composite$frequency[1:200],composite$mean[1:200])
plot(composite$frequency[1:200],composite$stddev[1:200])
plot(composite$frequency[1:200],composite$mean[1:200])
composite[composite$mean==0]
composite[composite$mean==0,]
?plot
plot(composite$frequency[1:200],composite$mean[200:1])
plot(composite$frequency[1:200],composite$stddev[1:200])
plot(composite$frequency[1:200],composite$stddev[1:200],type="h")
plot(composite$frequency[1:200],composite$stddev[1:200],xlim=rev(range(composite$frequency[1:200])))
composite <- ddply(singles,c("normalized"),summarize, # aggregate by normalized wordcount
frequency=length(sentiment),
mean=sum(sentiment)/length(sentiment),
stddev=sd(sentiment)
se=stddev/sqrt(frequency)
)
composite <- ddply(singles,c("normalized"),summarize, # aggregate by normalized wordcount
frequency=length(sentiment),
mean=sum(sentiment)/length(sentiment),
stddev=sd(sentiment),
se=stddev/sqrt(frequency)
)
View(composite)
composite <- composite[composite$normalized!="",] # ignore frequency of blanks
composite <- composite[with(composite, order(-frequency)), ] # sort descending
View(composite)
plot(composite$frequency[1:200],composite$se[1:200],
xlim=rev(range(composite$frequency[1:200]))
) # shows sentiment convergence for high-frequency words
plot(composite$frequency[2:200],composite$se[2:200],
xlim=rev(range(composite$frequency[2:200]))
) # shows sentiment convergence for high-frequency words
plot(composite$frequency[2:200],composite$se[2:200],
xlim=rev(range(composite$frequency[2:200])),type="h"
) # shows sentiment convergence for high-frequency words
composite$normalized[composite$frequency>20]
composite$normalized[composite$frequency>19]
composite$normalized[composite$frequency>18]
composite$normalized[composite$frequency>17]
composite$normalized[composite$frequency>16]
composite$normalized[composite$frequency>20]
composite$normalized[composite$frequency>15]
composite$normalized[composite$frequency>15 & composite$se<0.15]
composite$normalized[composite$frequency>10 & composite$se<0.15]
composite[composite$frequency>10 & composite$se<0.15,]
View(tweetData)
type(tweetData$date)
typeof(tweetData$date)
?date
date('04-12-2015')
as.date('04-12-2015')
as.Date('04-12-2015')
tweetData$date <- as.Date(tweetData$date)
plot(tweetData$date)
plot(table(tweetData$date))
table(tweetData$date)
as.Date('04-12-2015')
as.Date('04/12/2015')
as.Date('2015-04-12')
strsplit('04-12-2015','-')
?splice
?strcat
rev(strsplit('04-12-2015','-'))
rev(strsplit('04-12-2015','-')[[1]])
year <- strsplit(tweetData$date)[3]
year <- strsplit(tweetData$date),'-')[3]
year <- strsplit(tweetData$date,'-')[3]
year <- strsplit(toString(tweetData$date),'-')[3]
year
strsplit(toString(tweetData$date),'-')
tweetData <- read.csv("~/Documents/stockMartket/analysis/data/tweetData.csv", stringsAsFactors=FALSE)
View(tweetData)
tweetData$sentiment <- ifelse(tweetData$sentiment == "bullish",1,       # sentiment to int
ifelse(tweetData$sentiment=="bearish",-1,Inf))
tweetData$numWords <- sapply(strsplit(tweetData$tweetBody," +"),length) # count words
onlySentiment <- tweetData[tweetData$sentiment!=Inf,]                    # isolate labeled tweets
year <- strsplit(tweetData$date,'-')[3]
year
strsplit(tweetData$date,'-')
getThird <- function(l) return l[3]
getThird <- function(l) return(l[3])
sapply(strsplit(tweetData$date,'-'),getThird)
tweetData$year <- sapply(strsplit(tweetData$date,'-'),getThird)
table(tweetData$year)
plot(table(tweetData$year))
plot(table(tweetData$year),xlab="Year")
plot(table(tweetData$year),xlab="Year",ylab="Frequency")
plot(table(tweetData$year),xlab="Year",ylab="Frequency",title="tets")
?plot
plot(table(tweetData$year),xlab="Year",ylab="Frequency",main="tets")
plot(table(tweetData$year),xlab="Year",ylab="Frequency",main="Distribution of Post Year")
plot(table(onlySentiment$numWords))                                     # are similar
plot(table(tweetData$numWords))                                         # these two plots
plot(table(onlySentiment$numWords))                                     # are similar
plot(table(tweetData$numWords))                                         # these two plots
plot(table(tweetData$numWords),xlab="Number of words",ylab="Frequency",main="Distribution of Word Counts")                                         # these two plots
plot(composite$frequency[1:200],composite$se[1:200],
xlim=rev(range(composite$frequency[1:200]))
) # shows non-controversiality of high-frequency words
plot(composite$frequency[1:200],composite$se[1:200],
xlab="Frequency",
ylab="Standard Error",
main="Convergence of Sentiment on Common Words"
) # shows non-controversiality of high-frequency words
plot(composite$frequency[1:200],composite$se[1:200],
xlab="Frequency of Isolated Occurrence",
ylab="Standard Error",
main="Convergence of Sentiment on Common Words"
) # shows non-controversiality of high-frequency words
plot(composite$frequency[1:200],composite$se[1:200],
xlab="Frequency of Isolated Occurrence",
ylab="Standard Error",
main="Convergence of Sentiment for Common Words"
) # shows non-controversiality of high-frequency words
library(plyr)
tweetData <- read.csv("~/Documents/stockMartket/analysis/data/tweetData.csv", stringsAsFactors=FALSE)
tweetDataSentiment140 <- read.csv("~/Documents/stockMartket/analysis/data/tweetDataSentiment140.csv", stringsAsFactors=FALSE)
userStats <- read.csv("~/Documents/stockMartket/analysis/data/userStats.csv", stringsAsFactors=FALSE)
tweetData$sentiment <- ifelse(tweetData$sentiment == "bullish",1,       # sentiment to int
ifelse(tweetData$sentiment=="bearish",-1,Inf))
tweetData$numWords <- sapply(strsplit(tweetData$tweetBody," +"),length) # count words
switch <- function(r) return(paste(r[3],r[2],r[1],sep="-"))             # fix date encoding
year <- function(r) return(as.integer(r[1]))
tweetData$date <- sapply(strsplit(tweetData$date,'-'),switch)
tweetData$year <- sapply(strsplit(tweetData$date,'-'),year)
sen140<-tweetDataSentiment140[,c("tweetID","sentiment140")]             # add sentiment140
tweetData <- merge(tweetData,n,by="tweetID",all.tweetData=FALSE,all.sen140=FALSE)
tweetData <- tweetData[tweetData$year > 2011,] # only recent tweets
# extract list of stock tickers
tweetData$tickers <- sapply(rapply(mapply(strsplit,MoreArgs=.(" "), # horrendus
lapply(strsplit(gsub('[[:punct:]]', '',
toupper(tweetData$tweetBody)),"CASHTAG"),
function(l) l[0:-1])),function(r) r[1], how="list"),
function(r) unlist(r)[unlist(r)!=""&is.na(as.integer(unlist(r)))])
?contains
?strcontains
?substr
grepl(tweetData$tickers[1:10],"AAPL")
?grepl
grepl("AAPL",tweetData$tickers[1:10],)
grepl("AAPL",tweetData$tickers[1:100],)
tweetData$hasAAPL <- grepl("AAPL",tweetData$tickers,)
tweetData$hasTSLA <- grepl("TSLA",tweetData$tickers,)
tweetData$hasAMZN <- grepl("AMZN",tweetData$tickers,)
tweetData$hasTWTR <- grepl("TWTR",tweetData$tickers,)
tweetData$hasBABA <- grepl("BABA",tweetData$tickers,)
tweetData$hasKBIO <- grepl("KBIO",tweetData$tickers,)
View(tweetData)
tweetData[tweetData$year=="2015"][1:10]
tweetData[tweetData$year=="2015",][1:10]
a<-tweetData[tweetData$year=="2015",]
tweetData<-a<-
s
tweetData<-a
day <- function(r) return(as.integer(r[3]))
tweetData$day <- sapply(strsplit(tweetData$date,'-'),day)
View(tweetData)
sen140<-tweetDataSentiment140[,c("tweetID","sentiment140")]             # add sentiment140
tweetData <- merge(tweetData,n,by="tweetID",all.tweetData=FALSE,all.sen140=FALSE)
sen140<-tweetDataSentiment140[,c("tweetID","sentiment140")]             # add sentiment140
tweetData <- merge(tweetData,sen140,by="tweetID",all.tweetData=FALSE,all.sen140=FALSE)
View(tweetData)
byDay <- ddply(tweetData,c("day","userID","hasAAPL"),summarize, # aggregate by normalized wordcount
userSentimentAAPL <- sum(sentiment140)/count(sentiment140)
)
?ddply
arbDay <- tweetData[tweetData$date == "2015-01-07"]
arbDay <- tweetData[tweetData$date == "2015-01-07",]
View(arbDay)
byDay <- ddply(arbDay,c("userID","hasAAPL"),summarize, # aggregate by normalized wordcount
userSentimentAAPL <- sum(sentiment140)/count(sentiment140)
)
byDay <- ddply(arbDay,c("userID","hasAAPL"),summarise, # aggregate by normalized wordcount
userSentimentAAPL <- sum(sentiment140)/count(sentiment140)
)
RNTNclassification <- read.table("~/Documents/stockMartket/analysis/data/RNTNclassification.out", quote="\"")
View(RNTNclassification)
tweetData <- read.csv("~/Documents/stockMartket/analysis/data/tweetData.csv", stringsAsFactors=FALSE)
tweetDataSentiment140 <- read.csv("~/Documents/stockMartket/analysis/data/tweetDataSentiment140.csv", stringsAsFactors=FALSE)
userStats <- read.csv("~/Documents/stockMartket/analysis/data/userStats.csv", stringsAsFactors=FALSE)
tweetData$sentiment <- ifelse(tweetData$sentiment == "bullish",1,       # sentiment to int
ifelse(tweetData$sentiment=="bearish",-1,Inf))
switch <- function(r) return(paste(r[3],r[2],r[1],sep="-"))             # fix date encoding
year <- function(r) return(as.integer(r[1]))
day <- function(r) return(as.integer(r[3]))
tweetData$date <- sapply(strsplit(tweetData$date,'-'),switch)
tweetData$year <- sapply(strsplit(tweetData$date,'-'),year)
tweetData$day <- sapply(strsplit(tweetData$date,'-'),day)
sen140<-tweetDataSentiment140[,c("tweetID","sentiment140")]             # add sentiment140
tweetData <- merge(tweetData,sen140,by="tweetID",all.tweetData=FALSE,all.sen140=FALSE)
tweetData$RNTN <- RNTNclassification
length(tweetData[1:192347,])
nrow(tweetData[1:192347,])
tweetData<-tweetData[1:192347,]
tweetData$RNTN <- RNTNclassification
View(tweetData)
tweetData$RNTN <- as.factor(RNTNclassification)
as.factor(RNTNclassification)
typeof(RNTNclassification
)
as.vactor(RNTNclassification)
as.vector(RNTNclassification)
as.factor(as.vector(RNTNclassification))
tweeData$RNTN <- as.vector(RNTNclassification)
tweetData$RNTN <- as.vector(RNTNclassification)
View(tweetData)
RNTNclassification[1]
RNTNclassification[[1]]
tweetData$RNTN <- RNTNclassification[[1]]
View(tweetData)
tweetData <- tweetData[tweetData$year == 2015,] # only recent tweets
# extract list of stock tickers
tweetData$tickers <- sapply(rapply(mapply(strsplit,MoreArgs=.(" "), # horrendus
lapply(strsplit(gsub('[[:punct:]]', '',
toupper(tweetData$tweetBody)),"CASHTAG"),
function(l) l[0:-1])),function(r) r[1], how="list"),
function(r) unlist(r)[unlist(r)!=""&is.na(as.integer(unlist(r)))])
tweetData$hasAAPL <- grepl("AAPL",tweetData$tickers,)
View(tweetData)
tweetData$hasAMZN <- grepl("AMZN",tweetData$tickers,)
tweetData$date <- sapply(strsplit(tweetData$date,'-'),switch)
View(tweetData)
tweetData$date <- sapply(strsplit(tweetData$date,'-'),switch)
View(tweetData)
tweetData$month <- sapply(strsplit(tweetData$date,'-'),month)
month <- function(r) return(as.integer(r[2]))
tweetData$month <- sapply(strsplit(tweetData$date,'-'),month)
arbDay <- tweetData[tweetData$month=="6" & tweetData$day="12",]
arbDay <- tweetData[tweetData$month="6" & tweetData$day="12",]
arbDay <- tweetData[tweetData$month=="6" & tweetData$day=="12",]
View(arbDay)
byDay <- ddply(arbDay,c("userID","hasAAPL"),summarise, # aggregate by normalized wordcount
userSentimentAAPL <- sum(sentiment140)/count(sentiment140)
)
arbDay$userID <- as.factor(arbDay$userID)
arbDay$hasAAPL <- as.factor(arbDay$hasAAPL)
byDay <- ddply(arbDay,c("userID","hasAAPL"),summarise, # aggregate by normalized wordcount
userSentimentAAPL <- sum(sentiment140)/count(sentiment140)
)
byDay <- ddply(arbDay,c("userID","hasAAPL"),summarise, # aggregate by normalized wordcount
)
library(plyr)
tweetData <- read.csv("~/Documents/stockMartket/analysis/data/tweetData.csv", stringsAsFactors=FALSE)
tweetData$sentiment <- ifelse(tweetData$sentiment == "bullish",1,       # sentiment to int
ifelse(tweetData$sentiment=="bearish",-1,Inf))
View(tweetData)
RNTNclassification <- read.table("~/Documents/stockMartket/analysis/data/RNTNclassification.out", quote="\"")
View(RNTNclassification)
RNTNclassification <- read.table("~/Documents/stockMartket/analysis/data/RNTNclassification.out", quote="\"")
View(RNTNclassification)
View(tweetData)
labeled <- read.csv("~/Documents/stockMartket/analysis/data/labeled.txt", header=FALSE)
View(labeled)
labels <- labeled$V9
labels <- ifelse(labels=="bullish",1,ifelse(labels=="bearish",0,Inf))
View(labels)
RNTNclassification <- ifelse(RNTNclassification > 0, 1, ifelse(RNTNclassification < 0, -1, 0))
View(RNTNclassification)
table(labels)
labels[labels==0] = -1
View(labels)
table(labels)
table(RNTNclassification)
labeledTweets <- read.csv("~/Documents/stockMartket/analysis/data/labeledTweets.txt", header=FALSE)
View(labeledTweets)
labeledTweets$RNTN <- RNTNclassification
View(labeledTweets)
labeledTweets$V2 == labeledTweets$RNTN
labeledTweets[labeledTweets$V2 == labeledTweets$RNTN]
length(labeledTweets[labeledTweets$V2 == labeledTweets$RNTN])
length(labeledTweets[labeledTweets$V2 ~= labeledTweets$RNTN])
length(labeledTweets[labeledTweets$V2 != labeledTweets$RNTN])
length(labeledTweets)
length(labeledTweets[1,])
length(labeledTweets[,1])
labeledTweets$correct <- labeledTweets$V2 == labeledTweets$RNTN
View(labeledTweets)
labeledTweets[labeledTweets$correct]
lengtH(labeledTweets[labeledTweets$correct])
length(labeledTweets[labeledTweets$correct])
length(labeledTweets$correct)
length(labeledTweets[labeledTweets$correct,])
labeledTweets[labeledTweets$correct,][1:10,]
length(labeledTweets[labeledTweets$correct,][,1])
11333/15646
11333/15646
11333/15646
