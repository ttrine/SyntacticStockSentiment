{
    "contents" : "library(plyr)\n\ntweetData <- read.csv(\"~/Documents/stockMartket/analysis/data/tweetData.csv\", stringsAsFactors=FALSE)\ntweetDataSentiment140 <- read.csv(\"~/Documents/stockMartket/analysis/data/tweetDataSentiment140.csv\", stringsAsFactors=FALSE)\nuserStats <- read.csv(\"~/Documents/stockMartket/analysis/data/userStats.csv\", stringsAsFactors=FALSE)\n\ntweetData$sentiment <- ifelse(tweetData$sentiment == \"bullish\",1,       # sentiment to int\n                       ifelse(tweetData$sentiment==\"bearish\",-1,Inf))\n\ntweetData$numWords <- sapply(strsplit(tweetData$tweetBody,\" +\"),length) # count words\n\nswitch <- function(r) return(paste(r[3],r[2],r[1],sep=\"-\"))             # fix date encoding\nyear <- function(r) return(as.integer(r[1]))\nday <- function(r) return(as.integer(r[3]))\nmonth <- function(r) return(as.integer(r[2]))\n\ntweetData$date <- sapply(strsplit(tweetData$date,'-'),switch)\ntweetData$year <- sapply(strsplit(tweetData$date,'-'),year)\ntweetData$day <- sapply(strsplit(tweetData$date,'-'),day)\ntweetData$month <- sapply(strsplit(tweetData$date,'-'),month)\n\nsen140<-tweetDataSentiment140[,c(\"tweetID\",\"sentiment140\")]             # add sentiment140\ntweetData <- merge(tweetData,sen140,by=\"tweetID\",all.tweetData=FALSE,all.sen140=FALSE)\n\ntweetData <- tweetData[tweetData$year == 2015,] # only recent tweets\n\n# extract list of stock tickers\ntweetData$tickers <- sapply(rapply(mapply(strsplit,MoreArgs=.(\" \"), # horrendus\n                     lapply(strsplit(gsub('[[:punct:]]', '',\n                    toupper(tweetData$tweetBody)),\"CASHTAG\"),\n                     function(l) l[0:-1])),function(r) r[1], how=\"list\"),\n                     function(r) unlist(r)[unlist(r)!=\"\"&is.na(as.integer(unlist(r)))])\n\nlinkFollowers<-function(row) userStats$numFollowers[userStats$username == row]\nsapply(tweetData$userName,linkFollowers)[1:10]\n\nonlySentiment <- tweetData[tweetData$sentiment!=Inf,]         # isolate labeled tweets\n\nplot(table(tweetData$numWords),xlab=\"Number of words\",ylab=\"Frequency\",\n     main=\"Distribution of Word Counts\")\n\nplot(table(tweetData$year),xlab=\"Year\",ylab=\"Frequency\",main=\"Distribution of Post Year\")\n\nremNeutral <- function(s) s[s!=\"LINKREPLACE\" & !grepl('CASHTAG',s,fixed=TRUE)]\nonlySentiment$filteredWords <- sapply(sentimentWords, remNeutral)\nonlySentiment$filteredWordLens <- sapply(onlySentiment$filteredWords,length)\n\nsingles <- onlySentiment[onlySentiment$filteredWordLens==1,]\nsingles$normalized <- gsub('[[:punct:]]', '', tolower(singles$filteredWords))\n\ncomposite <- ddply(singles,c(\"normalized\"),summarize, # aggregate by normalized wordcount\n                   frequency=length(sentiment),\n                   mean=sum(sentiment)/length(sentiment),\n                   stddev=sd(sentiment),\n                   se=stddev/sqrt(frequency)\n)\ncomposite <- composite[composite$normalized!=\"\",] # ignore frequency of blanks\ncomposite <- composite[with(composite, order(-frequency)), ] # sort descending\nplot(composite$frequency[1:200],composite$se[1:200],\n     xlab=\"Frequency of Isolated Occurrence\",\n     ylab=\"Standard Error\",\n     main=\"Convergence of Sentiment for Common Words\"\n) # shows non-controversiality of high-frequency words\n\ntweetData$hasAAPL <- grepl(\"AAPL\",tweetData$tickers,)\ntweetData$hasAMZN <- grepl(\"AMZN\",tweetData$tickers,)\n\narbDay <- tweetData[tweetData$month==\"6\" & tweetData$day==\"12\",]\narbDay$userID <- as.factor(arbDay$userID)\narbDay$hasAAPL <- as.factor(arbDay$hasAAPL)\nbyDay <- ddply(arbDay,c(\"userID\",\"hasAAPL\"),summarise, # aggregate by normalized wordcount\n    userSentimentAAPL <- sum(sentiment140)/count(sentiment140)\n)\n\n\n",
    "created" : 1449900396617.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "2518548142",
    "id" : "B0B304D8",
    "lastKnownWriteTime" : 1450069505,
    "path" : "~/Documents/stockMartket/analysis/code/tweetAnalyses.R",
    "project_path" : "code/tweetAnalyses.R",
    "properties" : {
        "tempName" : "Untitled1"
    },
    "source_on_save" : false,
    "type" : "r_source"
}